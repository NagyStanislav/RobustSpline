% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hello.R
\name{IRLS}
\alias{IRLS}
\title{Iteratively Reweighted Least Squares for robust functional regression}
\usage{
IRLS(
  Z,
  Y,
  lambda,
  H,
  type,
  alpha = 1/2,
  w = NULL,
  sc = 1,
  resids.in = rep(1, length(Y)),
  tuning = NULL,
  toler = 1e-07,
  imax = 1000,
  vrs = "C",
  toler_solve = 1e-35
)
}
\arguments{
\item{Z}{Data matrix of dimension \code{n}-times-\code{p}, where \code{n} is
the number of observations, \code{p} is the dimension.}

\item{Y}{Vector of responses of length \code{n}.}

\item{lambda}{Tuning parameter, a non-negative real number.}

\item{H}{Penalty matrix of size \code{p}-times-\code{p} that
is used inside the quadratic term for penalizing estimated parameters.}

\item{type}{The type of the loss function used in the minimization problem.
Accepted are \code{type="absolute"} for the absolute loss \code{rho(t)=|t|/2};
\code{type="quantile"} for the (asymmetric) quantile loss 
\code{rho(t)=t(alpha-I[t<0])} (\code{absolute} loss with \code{alpha=1/2});
\code{type="square"} for the square loss \code{rho(t)=t^2}; 
\code{type="Huber"} for the Huber loss \code{rho(t)=t^2/2} if 
\code{|t|<tuning} and \code{rho(t)=tuning*(|t|-tuning/2)} otherwise; and 
\code{type="logistic"} for the logistic loss 
\code{rho(t)=2*t + 4*log(1+exp(-t))-4*log(2)}.}

\item{alpha}{The order of the quantile if \code{type="quantile"}. By default
taken to be \code{alpha=1/2}, which gives the absolute loss 
(\code{type="absolute"}).}

\item{w}{Vector of length \code{n} of weights attached to the elements of 
\code{Y}. If \code{w=NULL} (default), a constant vector with values 
\code{1/n} is used.}

\item{sc}{Scale parameter to be used in the IRLS. By default \code{sc=1}, 
that is no scaling is performed.}

\item{resids.in}{Initialization of the vector of residuals used to launch 
the IRLS algorithm.}

\item{tuning}{A non-negative tuning constant for the absolute/quantile loss 
function (that is, \code{type="absolute"} or \code{type="tuning"}). 
For \code{tuning = 0} the standard 
absolute loss \code{rho(t) = |t|/2} is used (or its asymmetric version for
the quantile loss). For \code{tuning > 0}, the Huber 
loss is used, that is \code{rho(t)} is quadratic for \code{|t|<tuning} and 
linear for \code{|t|>=tuning}. The function is chosen so that \code{rho} 
is always continuously differentiable.}

\item{toler}{A small positive constant specifying the tolerance level for 
terminating the algorithm. The procedure stops if the maximum absolute 
distance between the residuals in the previous iteration and the new 
residuals drops below \code{toler}.}

\item{imax}{Maximum number of allowed iterations of IRLS.}

\item{vrs}{Version of the algorhitm to be used. The program is prepared in
two versions: i) \code{vrs="C"} calls the \code{C++} version of the 
algorithm, programmed within the \code{RCppArmadillo} framework for
manipulating matrices. This is typically the fastest version. 
ii) \code{vrs="R"} calls the \code{R} version. The two versions may 
give slightly different results due to the differences in evaluating inverse
matrices. With \code{vrs="C"} one uses the function \code{solve} directly
from \code{Armadillo} library in \code{C++}; with \code{vrs="R"} the 
standard function \code{solve} from \code{R} package \code{base} is used 
with the option \code{tol = toler_solve}.}

\item{toler_solve}{A small positive constant to be passed to function
\link[base]{solve} as argument \code{tol}. Used to handle numerically 
singular matrices whose inverses need to be approximated. By default set to
1e-35.}
}
\value{
A list composed of:
\itemize{
 \item{"theta_hat"}{ A numerical matrix of size \code{p}-times-\code{1} of 
 estimated regression coefficients.}
 \item{"converged"}{ Indicator whether the IRLS procedure succefully 
 converged. Takes value 1 if IRLS converged, 0 otherwise.}
 \item{"ic"}{ Number of iterations needed to reach connvergence. If 
 \code{converged=0}, always \code{ic=imax}.}
 \item{"resids"}{ A numerical vecotor of length \code{n} containing the final
 set of residuals in the fit of \code{Y} on \code{Z}.}
 \item{"hat_values"}{ Diagonal terms of the (possibly penalized) hat matrix of
 the form \code{Z*solve(t(Z)*W*Z+n*lambda*H)*t(Z)*W}, where \code{W} 
 is the diagonal weight matrix in the final iteration of IRLS.}
 \item{"last_check"}{ The final maximum absolute difference between 
 \code{resids} and the residuals from the previous iteration. We have 
 \code{resids < toler} if and only if the IRLS converged (that is, 
 \code{converged=1}).}
 \item{"weights"}{ The vector of weights given to the observations in the 
 final iteration of IRLS. For squared loss (\code{type="square"}) this gives 
 a vector whose all elements are 2.}
 \item{"fitted"}{ Fitted values in the model. A vector of length \code{n} 
 correponding to the fits of \code{Y}.}
}
}
\description{
Iteratively Reweighted Least Squares (IRLS) algorithm that is used to 
estimate a vector of regression parameters in a (possibly robust and 
penalized) linear regression model. Weights can be supplied as well.
}
\details{
Especially for extremely small values of \code{lambda}, numerically
singular matrices must be inverted in the procedure. This may cause numerical
instabilites, and is the main cause for differences in results when using
\code{vrs="C"} and \code{vrs="R"}. In case when IRLS does not converge within
\code{imax} iterations, a warning is given.
}
\examples{
n = 50      # sample size
p = 10      # dimension of predictors
Z = matrix(rnorm(n*p),ncol=p) # design matrix
Y = Z[,1]   # response vector
lambda = 1  # tuning parameter for penalization
H = diag(p) # penalty matrix
type = "absolute" # absolute loss

# Run the two versions of the IRLS procedure
res_C = IRLS(Z, Y, lambda, H, type, vrs="C")
res_R = IRLS(Z, Y, lambda, H, type, vrs="R")
# Check whether both versions converged after the same number of iterations
res_C$ic
res_R$ic
# Check the maximum absolute difference between the results
max(abs(res_C$theta_hat-res_R$theta_hat))
# Visualise the difference between the results
plot(res_C$theta_hat ~ res_R$theta_hat)
}
\references{
Ioannis Kalogridis and Stanislav Nagy. (2025). Robust functional regression 
with discretely sampled predictors. 
\emph{Computational Statistics and Data Analysis}, to appear.

Peter. J. Huber. (1981). Robust Statistics, \emph{New York: John Wiley.}
}
\seealso{
\link{ridge} for a faster (non-robust) version of this
function with \code{type="square"}.
}
